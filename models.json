{
  "providers": [
    {
      "name": "Mock",
      "displayName": "Mock",
      "base_model_name": "mock",
      "credentials_needed": {},
      "models": [
        {
          "displayName": "Mock Model",
          "submodels": [
            "mock"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 2.5e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        }
      ]
    },
    {
      "name": "OpenAI",
      "displayName": "OpenAI",
      "base_model_name": "openai",
      "credentials_needed": {
        "api_key": {
          "type": "string",
          "displayName": "Chave de API",
          "description": "Chave de API da OpenAI",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "GPT-4o",
          "submodels": [
            "gpt-4o",
            "gpt-4o-2024-08-06",
            "gpt-4o-2024-05-13",
            "gpt-4o-2024-11-20"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 2.5e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT-4o mini",
          "submodels": [
            "gpt-4o-mini",
            "gpt-4o-mini-2024-07-18",
            "gpt-4o-mini-2024-08-06",
            "gpt-4o-mini-2024-05-13"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT-4 Turbo",
          "submodels": [
            "gpt-4-turbo",
            "gpt-4-turbo-2024-04-09"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1e-05,
          "output_cost_per_token": 3e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT-4 Turbo Preview",
          "submodels": [
            "gpt-4-turbo-preview",
            "gpt-4-1106-preview",
            "gpt-4-0125-preview"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1e-05,
          "output_cost_per_token": 3e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GPT-4",
          "submodels": [
            "gpt-4",
            "gpt-4-0314",
            "gpt-4-0613"
          ],
          "max_input_tokens": 8192,
          "max_output_tokens": 4096,
          "input_cost_per_token": 3e-05,
          "output_cost_per_token": 6e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GPT-4 32k",
          "submodels": [
            "gpt-4-32k",
            "gpt-4-32k-0314",
            "gpt-4-32k-0613"
          ],
          "max_input_tokens": 32768,
          "max_output_tokens": 4096,
          "input_cost_per_token": 6e-05,
          "output_cost_per_token": 0.00012,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GPT-4 Vision",
          "submodels": [
            "gpt-4-vision-preview",
            "gpt-4-1106-vision-preview"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1e-05,
          "output_cost_per_token": 3e-05,
          "supports_function_calling": false,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT-3.5 Turbo",
          "submodels": [
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-0301",
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-0125"
          ],
          "max_input_tokens": 4097,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-06,
          "output_cost_per_token": 2e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GPT-3.5 Turbo 16k",
          "submodels": [
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo-16k-0613"
          ],
          "max_input_tokens": 16385,
          "max_output_tokens": 4096,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 4e-06,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "o1",
          "submodels": [
            "o1",
            "o1-2024-12-17"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 100000,
          "input_cost_per_token": 1.5e-05,
          "output_cost_per_token": 6e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "o1-preview",
          "submodels": [
            "o1-preview",
            "o1-preview-2024-09-12"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 1.5e-05,
          "output_cost_per_token": 6e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "o1-mini",
          "submodels": [
            "o1-mini",
            "o1-mini-2024-09-12"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 65536,
          "input_cost_per_token": 1.1e-06,
          "output_cost_per_token": 4.4e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "o3-mini",
          "submodels": [
            "o3-mini",
            "o3-mini-2025-01-31"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 100000,
          "input_cost_per_token": 1.1e-06,
          "output_cost_per_token": 4.4e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "chatgpt-4o-latest",
          "submodels": [
            "chatgpt-4o-latest"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 5e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 4.1",
          "submodels": [
            "gpt-4.1",
            "gpt-4.1-2025-04-14"
          ],
          "max_input_tokens": 1047576,
          "max_output_tokens": 32768,
          "input_cost_per_token": 2e-06,
          "output_cost_per_token": 8e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 4.1 Mini",
          "submodels": [
            "gpt-4.1-mini",
            "gpt-4.1-mini-2025-04-14"
          ],
          "max_input_tokens": 1047576,
          "max_output_tokens": 32768,
          "input_cost_per_token": 4e-07,
          "output_cost_per_token": 1.6e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 4.1 Nano",
          "submodels": [
            "gpt-4.1-nano",
            "gpt-4.1-nano-2025-04-14"
          ],
          "max_input_tokens": 1047576,
          "max_output_tokens": 32768,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 4e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5",
          "submodels": [
            "gpt-5",
            "gpt-5-2025-08-07"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 1.25e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5 Mini",
          "submodels": [
            "gpt-5-mini",
            "gpt-5-mini-2025-08-07"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 2.5e-07,
          "output_cost_per_token": 2e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5 Nano",
          "submodels": [
            "gpt-5-nano",
            "gpt-5-nano-2025-08-07"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 5e-08,
          "output_cost_per_token": 4e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5.2",
          "submodels": [
            "gpt-5.2",
            "gpt-5.2-2025-12-11"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 1.75e-06,
          "output_cost_per_token": 1.4e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        }
      ]
    },
    {
      "name": "Bedrock",
      "displayName": "AWS Bedrock",
      "base_model_name": "bedrock",
      "credentials_needed": {
        "aws_region_name": {
          "type": "string",
          "displayName": "Nome da Região",
          "description": "Nome da Região da AWS. Por exemplo: us-east-1, sa-east-1, etc.",
          "required": true
        },
        "aws_access_key_id": {
          "type": "string",
          "displayName": "Access Key ID",
          "description": "AWS Access Key ID da conta com permissão para usar o Bedrock. Garanta que deu as permissões corretas no IAM.",
          "required": true
        },
        "aws_secret_access_key": {
          "type": "string",
          "displayName": "Secret Access Key",
          "description": "Secret Access Key da AWS ",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Mistral 7B Instruct",
          "submodels": [
            "mistral.mistral-7b-instruct-v0:2"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Mixtral 8x7B Instruct",
          "submodels": [
            "mistral.mixtral-8x7b-instruct-v0:1"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 4.5e-07,
          "output_cost_per_token": 7e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Mistral Large",
          "submodels": [
            "mistral.mistral-large-2402-v1:0"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 8e-06,
          "output_cost_per_token": 2.4e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Mistral Small",
          "submodels": [
            "mistral.mistral-small-2402-v1:0"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1e-06,
          "output_cost_per_token": 3e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Claude 3 Sonnet",
          "submodels": [
            "us.anthropic.claude-3-sonnet-20240229-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude 3.5 Sonnet",
          "submodels": [
            "us.anthropic.claude-3-5-sonnet-20240620-v1:0",
            "us.anthropic.claude-3-5-sonnet-20241022-v2:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude 3 Haiku",
          "submodels": [
            "us.anthropic.claude-3-haiku-20240307-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 2.5e-07,
          "output_cost_per_token": 1.25e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude 3.5 Haiku",
          "submodels": [
            "us.anthropic.claude-3-5-haiku-20241022-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 8e-07,
          "output_cost_per_token": 4e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Claude 3 Opus",
          "submodels": [
            "us.anthropic.claude-3-opus-20240229-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-05,
          "output_cost_per_token": 7.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude 3.7 Sonnet",
          "submodels": [
            "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude Opus 4.5",
          "submodels": [
            "arn:aws:bedrock:us-east-1:605575731210:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 5e-06,
          "output_cost_per_token": 2.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude Haiku 4.5",
          "submodels": [
            "arn:aws:bedrock:us-east-1:605575731210:inference-profile/global.anthropic.claude-haiku-4-5-20251001-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 1e-06,
          "output_cost_per_token": 5e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude Sonnet 4.5",
          "submodels": [
            "arn:aws:bedrock:us-east-1:605575731210:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude Sonnet 4",
          "submodels": [
            "arn:aws:bedrock:us-east-1:605575731210:inference-profile/global.anthropic.claude-sonnet-4-20250514-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Claude Opus 4.1",
          "submodels": [
            "anthropic.claude-opus-4-1-20250805-v1:0"
          ],
          "max_input_tokens": 200000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 5e-06,
          "output_cost_per_token": 2.5e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Llama3 8B Instruct",
          "submodels": [
            "meta.llama3-8b-instruct-v1:0"
          ],
          "max_input_tokens": 8192,
          "max_output_tokens": 8192,
          "input_cost_per_token": 3e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 70B Instruct",
          "submodels": [
            "meta.llama3-70b-instruct-v1:0"
          ],
          "max_input_tokens": 8192,
          "max_output_tokens": 8192,
          "input_cost_per_token": 2.65e-06,
          "output_cost_per_token": 3.5e-06,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 1-8B Instruct",
          "submodels": [
            "us.meta.llama3-1-8b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 2.2e-07,
          "output_cost_per_token": 2.2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 1-70B Instruct",
          "submodels": [
            "us.meta.llama3-1-70b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 9.9e-07,
          "output_cost_per_token": 9.9e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 2-1B Instruct",
          "submodels": [
            "us.meta.llama3-2-1b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 2-3B Instruct",
          "submodels": [
            "us.meta.llama3-2-3b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 1.5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Llama3 2-11B Instruct",
          "submodels": [
            "us.meta.llama3-2-11b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 3.5e-07,
          "output_cost_per_token": 3.5e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Llama3 2-90B Instruct",
          "submodels": [
            "us.meta.llama3-2-90b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 2e-06,
          "output_cost_per_token": 2e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Llama 4 Maverick 17B",
          "submodels": [
            "meta.llama4-maverick-17b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 2.4e-07,
          "output_cost_per_token": 9.7e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Llama 4 Scout 17B",
          "submodels": [
            "meta.llama4-scout-17b-instruct-v1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 1.7e-07,
          "output_cost_per_token": 6.6e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "OpenAI GPT OSS 120B",
          "submodels": [
            "openai.gpt-oss-120b-1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1e-6,
          "output_cost_per_token": 3e-6,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "OpenAI GPT OSS 20B",
          "submodels": [
            "openai.gpt-oss-20b-1:0"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1e-6,
          "output_cost_per_token": 3e-6,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 32B",
          "submodels": [
            "qwen.qwen3-32b-v1:0"
          ],
          "max_input_tokens": 16384,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1.5e-7,
          "output_cost_per_token": 6e-7,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 Coder 30B",
          "submodels": [
            "qwen.qwen3-coder-30b-a3b-v1:0"
          ],
          "max_input_tokens": 262144,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1.5e-7,
          "output_cost_per_token": 6e-7,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 Next 80B A3B",
          "submodels": [
            "qwen.qwen3-next-80b-a3b"
          ],
          "max_input_tokens": 262144,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1.5e-7,
          "output_cost_per_token": 1.2e-6,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Ministral 14B 3.0",
          "submodels": [
            "mistral.ministral-3-14b-instruct"
          ],
          "max_input_tokens": 262144,
          "max_output_tokens": 8191,
          "input_cost_per_token": 2e-7,
          "output_cost_per_token": 2e-7,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "NVIDIA Nemotron Nano 2",
          "submodels": [
            "nvidia.nemotron-nano-9b-v2"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 6e-8,
          "output_cost_per_token": 2.3e-7,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "NVIDIA Nemotron Nano 2 VL",
          "submodels": [
            "nvidia.nemotron-nano-12b-v2"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 2e-7,
          "output_cost_per_token": 6e-7,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Nemotron Nano 3 30B",
          "submodels": [
            "nvidia.nemotron-nano-3-30b"
          ],
          "max_input_tokens": 262144,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1.5e-7,
          "output_cost_per_token": 6e-7,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        }
      ]
    },
    {
      "name": "Azure AI",
      "displayName": "Azure",
      "base_model_name": "azure_ai",
      "credentials_needed": {
        "api_key": {
          "type": "string",
          "displayName": "Chave de API",
          "description": "Chave de API da Azure AI",
          "required": true
        },
        "base_url": {
          "type": "string",
          "displayName": "URL Base",
          "description": "Base URL da Azure AI",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Deepseek",
          "submodels": [
            "deepseek-r1",
            "DeepSeek-V3.1"            
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 1.35e-06,
          "output_cost_per_token": 5.4e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Deepseek V3",
          "submodels": [
            "deepseek-v3"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 1.35e-06,
          "output_cost_per_token": 5.4e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Jamba Instruct",
          "submodels": [
            "jamba-instruct"
          ],
          "max_input_tokens": 70000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 5e-07,
          "output_cost_per_token": 7e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Mistral Nemo",
          "submodels": [
            "mistral-nemo"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 1.5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Mistral Large",
          "submodels": [
            "mistral-large",
            "mistral-large-2407"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 4e-06,
          "output_cost_per_token": 1.2e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Mistral Small",
          "submodels": [
            "mistral-small",
            "mistral-small-2503"
          ],
          "max_input_tokens": 32000,
          "max_output_tokens": 8191,
          "input_cost_per_token": 1e-06,
          "output_cost_per_token": 3e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Llama-3.2 11B Vision Instruct",
          "submodels": [
            "Llama-3.2-11B-Vision-Instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 3.7e-07,
          "output_cost_per_token": 3.7e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Llama-3.3 70B Instruct",
          "submodels": [
            "Llama-3.3-70B-Instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 7.1e-07,
          "output_cost_per_token": 7.1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Meta-Llama-3 70B Instruct",
          "submodels": [
            "Meta-Llama-3-70B-Instruct"
          ],
          "max_input_tokens": 8192,
          "max_output_tokens": 2048,
          "input_cost_per_token": 1.1e-06,
          "output_cost_per_token": 3.7e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Meta-Llama-3.1 8B Instruct",
          "submodels": [
            "Meta-Llama-3.1-8B-Instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 3e-07,
          "output_cost_per_token": 6.1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Meta-Llama-3.1 70B Instruct",
          "submodels": [
            "Meta-Llama-3.1-70B-Instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 2.68e-06,
          "output_cost_per_token": 3.54e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Meta-Llama-3.1 405B Instruct",
          "submodels": [
            "Meta-Llama-3.1-405B-Instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 2048,
          "input_cost_per_token": 5.33e-06,
          "output_cost_per_token": 1.6e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-4-mini-instruct",
          "submodels": [
            "Phi-4-mini-instruct"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 4096,
          "input_cost_per_token": 0,
          "output_cost_per_token": 0,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Phi-4-multimodal-instruct",
          "submodels": [
            "Phi-4-multimodal-instruct"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 4096,
          "input_cost_per_token": 0,
          "output_cost_per_token": 0,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Phi-4",
          "submodels": [
            "Phi-4"
          ],
          "max_input_tokens": 16384,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1.25e-07,
          "output_cost_per_token": 5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Phi-3.5-mini-instruct",
          "submodels": [
            "Phi-3.5-mini-instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.3e-07,
          "output_cost_per_token": 5.2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3.5-vision-instruct",
          "submodels": [
            "Phi-3.5-vision-instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.3e-07,
          "output_cost_per_token": 5.2e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3.5-MoE-instruct",
          "submodels": [
            "Phi-3.5-MoE-instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.6e-07,
          "output_cost_per_token": 6.4e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3-mini-4k-instruct",
          "submodels": [
            "Phi-3-mini-4k-instruct"
          ],
          "max_input_tokens": 4096,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.3e-07,
          "output_cost_per_token": 5.2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3-mini-128k-instruct",
          "submodels": [
            "Phi-3-mini-128k-instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.3e-07,
          "output_cost_per_token": 5.2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3-small-8k-instruct",
          "submodels": [
            "Phi-3-small-8k-instruct"
          ],
          "max_input_tokens": 8192,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "Phi-3-small-128k-instruct",
          "submodels": [
            "Phi-3-small-128k-instruct"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 4096,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "GPT-4o",
          "submodels": [
            "gpt-4o"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 2.5e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT-4o mini",
          "submodels": [
            "gpt-4o-mini"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5",
          "submodels": [
            "gpt-5"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 1.25e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5 Mini",
          "submodels": [
            "gpt-5-mini"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 2.5e-07,
          "output_cost_per_token": 2e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "GPT 5 Nano",
          "submodels": [
            "gpt-5-nano"
          ],
          "max_input_tokens": 400000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 5e-08,
          "output_cost_per_token": 4e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        }
      ]
    },
    {
      "name": "Google AI",
      "displayName": "Google AI",
      "base_model_name": "gemini",
      "credentials_needed": {
        "api_key": {
          "type": "string",
          "displayName": "Chave de API",
          "description": "Chaven de API do Estúdio de IA da Google",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Gemini 2.0 Flash",
          "submodels": [
            "gemini-2.0-flash",
            "gemini-2.0-flash-001"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 4e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 2.0 Flash Lite Preview",
          "submodels": [
            "gemini-2.0-flash-lite-preview-02-05"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 7.5e-08,
          "output_cost_per_token": 3e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 2.0 Flash Lite",
          "submodels": [
            "gemini-2.0-flash-lite"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 7.5e-08,
          "output_cost_per_token": 3e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 2.0 Flash Exp",
          "submodels": [
            "gemini-2.0-flash-exp"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 0,
          "output_cost_per_token": 0,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 2.0 Flash Thinking Exp",
          "submodels": [
            "gemini-2.0-flash-thinking-exp",
            "gemini-2.0-flash-thinking-exp-01-21"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 0,
          "output_cost_per_token": 0,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 1.5 Flash",
          "submodels": [
            "gemini-1.5-flash",
            "gemini-1.5-flash-002",
            "gemini-1.5-flash-001",
            "gemini-1.5-flash-latest"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 7.5e-08,
          "output_cost_per_token": 3e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 1.5 Flash 8B",
          "submodels": [
            "gemini-1.5-flash-8b",
            "gemini-1.5-flash-8b-exp-0924",
            "gemini-1.5-flash-8b-exp-0827"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 8192,
          "input_cost_per_token": 0,
          "output_cost_per_token": 0,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 1.5 Pro",
          "submodels": [
            "gemini-1.5-pro",
            "gemini-1.5-pro-002",
            "gemini-1.5-pro-001",
            "gemini-1.5-pro-exp-0801",
            "gemini-1.5-pro-exp-0827",
            "gemini-1.5-pro-latest"
          ],
          "max_input_tokens": 2097152,
          "max_output_tokens": 8192,
          "input_cost_per_token": 3.5e-06,
          "output_cost_per_token": 1.05e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini Pro Vision",
          "submodels": [
            "gemini-pro-vision"
          ],
          "max_input_tokens": 30720,
          "max_output_tokens": 2048,
          "input_cost_per_token": 3.5e-07,
          "output_cost_per_token": 1.05e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini Gemma 2",
          "submodels": [
            "gemini-gemma-2-27b-it"
          ],
          "max_input_tokens": null,
          "max_output_tokens": 8192,
          "input_cost_per_token": 3.5e-07,
          "output_cost_per_token": 1.05e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini Gemma 2 (9B)",
          "submodels": [
            "gemini-gemma-2-9b-it"
          ],
          "max_input_tokens": null,
          "max_output_tokens": 8192,
          "input_cost_per_token": 3.5e-07,
          "output_cost_per_token": 1.05e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 2.5 Pro Preview",
          "submodels": [
            "gemini-2.5-pro-preview-05-06",
            "gemini-2.5-pro-preview-03-25"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65535,
          "input_cost_per_token": 1.25e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 2.5 Flash Preview",
          "submodels": [
            "gemini-2.5-flash-preview-05-20",
            "gemini-2.5-flash-preview-04-17"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65535,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "deprecated"
        },
        {
          "displayName": "Gemini 2.5 Pro",
          "submodels": [
            "gemini-2.5-pro"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 1.25e-06,
          "output_cost_per_token": 1e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 2.5 Flash",
          "submodels": [
            "gemini-2.5-flash"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 3e-07,
          "output_cost_per_token": 2.5e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 2.5 Flash Lite",
          "submodels": [
            "gemini-2.5-flash-lite"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 4e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 3 Pro Preview",
          "submodels": [
            "gemini-3-pro-preview"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 2e-06,
          "output_cost_per_token": 1.2e-05,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Gemini 3 Flash Preview",
          "submodels": [
            "gemini-3-flash-preview"
          ],
          "max_input_tokens": 1048576,
          "max_output_tokens": 65536,
          "input_cost_per_token": 5e-07,
          "output_cost_per_token": 3e-06,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        }
      ]
    },
    {
      "name": "Fireworks AI",
      "displayName": "AumoGPT",
      "base_model_name": "fireworks_ai",
      "credentials_needed": {
        "api_key": {
          "type": "string",
          "displayName": "Chave de API",
          "description": "Chave de API do AumoGPT. Entrar em contato com o suporte da AUMO para obter uma chave de API.",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Llama-3.1 8B",
          "submodels": [
            "llama-v3p1-8b-instruct"
          ],
          "max_input_tokens": 16384,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Deepseek v3",
          "submodels": [
            "deepseek-v3"
          ],
          "max_input_tokens": 128000,
          "max_output_tokens": 8192,
          "input_cost_per_token": 9e-07,
          "output_cost_per_token": 9e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "deprecated"
        },
        {
          "displayName": "GPT OSS 20B",
          "submodels": [
            "gpt-oss-20b"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 5e-08,
          "output_cost_per_token": 2e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GPT OSS 120b",
          "submodels": [
            "gpt-oss-120b"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 1.5e-07,
          "output_cost_per_token": 6e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Sabia 70b",
          "submodels": [
            "accounts/admin-d353ad/deployedModels/sabia-70b-v3-wstoaf4b"
          ],
          "max_input_tokens": 30720,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "AumoGPT 70b",
          "submodels": [
            "accounts/admin-d353ad/deployedModels/aumogpt-70b-v1-cezpimyl"
          ],
          "max_input_tokens": 30720,
          "max_output_tokens": 16384,
          "input_cost_per_token": 1e-07,
          "output_cost_per_token": 1e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Deepseek V3.1 Terminus",
          "submodels": [
            "deepseek-v3p1-terminus"
          ],
          "max_input_tokens": 160000,
          "max_output_tokens": 160000,
          "input_cost_per_token": 5.6e-07, 
          "output_cost_per_token": 1.68e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "GLM 4.6",
          "submodels": [
            "glm-4p6"
          ],
          "max_input_tokens": 198000,
          "max_output_tokens": 198000,
          "input_cost_per_token": 5.5e-07, 
          "output_cost_per_token": 2.19e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Kimi K2 Instruct",
          "submodels": [
            "kimi-k2-instruct-0905"
          ],
          "max_input_tokens": 256000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 6e-07,
          "output_cost_per_token": 2.5e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "DeepSeek V3.1",
          "submodels": [
            "deepseek-v3p1"
          ],
          "max_input_tokens": 160000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 5.6e-07,
          "output_cost_per_token": 1.68e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 235B A22B Thinking 2507",
          "submodels": [
            "qwen3-235b-a22b-thinking-2507"
          ],
          "max_input_tokens": 256000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 2.2e-07,
          "output_cost_per_token": 8.8e-07,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 Coder 480B A35B Instruct",
          "submodels": [
            "qwen3-coder-480b-a35b-instruct"
          ],
          "max_input_tokens": 256000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 4.5e-07,
          "output_cost_per_token": 1.8e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Qwen3 VL 235B A22B Instruct",
          "submodels": [
            "qwen3-vl-235b-a22b-instruct"
          ],
          "max_input_tokens": 256000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 2.2e-07,
          "output_cost_per_token": 8.8e-07,
          "supports_function_calling": true,
          "supports_vision": true,
          "status": "active"
        },
        {
          "displayName": "Deepseek v3.2",
          "submodels": [
            "deepseek-v3p2"
          ],
          "max_input_tokens": 160000,
          "max_output_tokens": 32768,
          "input_cost_per_token": 5.6e-07,
          "output_cost_per_token": 1.68e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        }
      ]
    },
    {
      "name": "OCI",
      "displayName": "Oracle OCI",
      "base_model_name": "oci",
      "credentials_needed": {
        "key": {
          "type": "string",
          "displayName": "Chave Privada",
          "description": "Private Key da Oracle",
          "required": true
        },
        "user": {
          "type": "string",
          "displayName": "Usuário",
          "description": "ID do Usuário da Oracle",
          "required": true
        },
        "region": {
          "type": "string",
          "displayName": "Região",
          "description": "Nome da Região da Oracle",
          "required": true
        },
        "tenancy": {
          "type": "string",
          "displayName": "Tenancy",
          "description": "O ID do Tenant da Oracle",
          "required": true
        },
        "fingerprint": {
          "type": "string",
          "displayName": "Fingerprint",
          "description": "Oracle Fingerprint",
          "required": true
        },
        "compartment_id": {
          "type": "string",
          "displayName": "Compartment ID",
          "description": "Oracle Compartment ID",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Grok 4",
          "submodels": [
            "xai.grok-4"
          ],
          "max_tokens": 128000,
          "max_input_tokens": 128000,
          "max_output_tokens": 128000,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Grok 3",
          "submodels": [
            "xai.grok-3"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 3e-06,
          "output_cost_per_token": 1.5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Grok 3 Fast",
          "submodels": [
            "xai.grok-3-fast"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 5e-06,
          "output_cost_per_token": 2.5e-05,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Grok 3 Mini",
          "submodels": [
            "xai.grok-3-mini"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 3e-07,
          "output_cost_per_token": 5e-07,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        },
        {
          "displayName": "Grok 3 Mini Fast",
          "submodels": [
            "xai.grok-3-mini-fast"
          ],
          "max_input_tokens": 131072,
          "max_output_tokens": 131072,
          "input_cost_per_token": 6e-07,
          "output_cost_per_token": 4e-06,
          "supports_function_calling": true,
          "supports_vision": false,
          "status": "active"
        }
      ]
    },
    {
      "name": "AUMO",
      "displayName": "AUMO Neuron",
      "base_model_name": "openai",
      "credentials_needed": {
        "api_key": {
          "type": "string",
          "displayName": "Chave de API",
          "description": "Chave de API da AUMO",
          "required": true
        }
      },
      "models": [
        {
          "displayName": "Avaliador de Ideia - Qwen3-4B",
          "submodels": [
            "qwen3-4b-finetuned"
          ],
          "max_tokens": 4096,
          "max_input_tokens": 4096,
          "max_output_tokens": 4096,
          "input_cost_per_token": 0.0,
          "output_cost_per_token": 0.0,
          "supports_function_calling": false,
          "supports_vision": false,
          "status": "active"
        }
      ]
    }
  ]
}
